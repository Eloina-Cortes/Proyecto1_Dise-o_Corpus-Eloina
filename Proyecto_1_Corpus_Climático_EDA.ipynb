{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e3f3e3",
   "metadata": {},
   "source": [
    "# Proyecto 1: Corpus Textual de Datos Climáticos de Oaxaca\n",
    "## Análisis Exploratorio Completo (EDA)\n",
    "\n",
    "**Objetivo**: Procesamiento y análisis riguroso de corpus multiregional de datos climáticos históricos de Oaxaca, México (junio-diciembre 2025).\n",
    "\n",
    "**Período**: 01/06/2025 - 31/12/2025 (214 días)  \n",
    "**Ciudades**: 8 regiones de Oaxaca  \n",
    "**Documentos**: 1,712 (8 × 214)  \n",
    "**Palabras totales**: 61,816  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535ef0c1",
   "metadata": {},
   "source": [
    "## 1. Importar Librerías y Cargar Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369af6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Configurar estilo\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Descargar recursos NLTK\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "SPANISH_STOP = set(stopwords.words(\"spanish\"))\n",
    "DOMAIN_STOP = {\"ciudad\", \"país\", \"clima\", \"temperatura\", \"humedad\"}\n",
    "ALL_STOP = SPANISH_STOP | DOMAIN_STOP\n",
    "\n",
    "print(\"✓ Librerías importadas exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01711e08",
   "metadata": {},
   "source": [
    "### 1.1 Cargar corpus JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102b3fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path):\n",
    "    \"\"\"Carga corpus en formato JSONL.\"\"\"\n",
    "    docs = []\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            try:\n",
    "                docs.append(json.loads(line))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Warning: línea {line_num} inválida: {e}\")\n",
    "    return docs\n",
    "\n",
    "# Cargar corpus\n",
    "corpus_path = \"data/corpus_weather.jsonl\"\n",
    "print(f\"Cargando corpus desde: {corpus_path}\")\n",
    "docs = load_jsonl(corpus_path)\n",
    "print(f\"✓ {len(docs)} documentos cargados\\n\")\n",
    "\n",
    "# Mostrar documento ejemplo\n",
    "print(\"EJEMPLO DE DOCUMENTO:\")\n",
    "print(\"-\" * 80)\n",
    "example = docs[0]\n",
    "print(f\"ID: {example['id']}\")\n",
    "print(f\"Ciudad: {example['city']} ({example['region']})\")\n",
    "print(f\"Fecha: {example['date']}\")\n",
    "print(f\"Texto:\\n{example['text'][:150]}...\\n\")\n",
    "print(f\"Metadatos: Temp.máx={example['metadata']['temp_max_celsius']}°C, \"\n",
    "      f\"Lluvia={example['metadata']['precipitation_mm']}mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a415e511",
   "metadata": {},
   "source": [
    "## 2. Estadísticas Básicas del Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e651395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_stats(docs):\n",
    "    \"\"\"Calcula estadísticas de longitud.\"\"\"\n",
    "    if not docs:\n",
    "        return {\"n_docs\": 0}\n",
    "    \n",
    "    lengths = [len(d[\"text\"].split()) for d in docs]\n",
    "    return {\n",
    "        \"n_docs\": len(docs),\n",
    "        \"total_words\": sum(lengths),\n",
    "        \"min_len\": min(lengths),\n",
    "        \"max_len\": max(lengths),\n",
    "        \"mean_len\": np.mean(lengths),\n",
    "        \"median_len\": np.median(lengths),\n",
    "        \"std_len\": np.std(lengths)\n",
    "    }\n",
    "\n",
    "stats = basic_stats(docs)\n",
    "\n",
    "print(\"ESTADÍSTICAS BÁSICAS DEL CORPUS\")\n",
    "print(\"=\" * 60)\n",
    "for key, val in stats.items():\n",
    "    if isinstance(val, float):\n",
    "        print(f\"  {key:20s}: {val:10.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key:20s}: {val:10d}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fa373a",
   "metadata": {},
   "source": [
    "## 3. Análisis por Ciudad y Región"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff2cda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrame para análisis\n",
    "df = pd.DataFrame([{\n",
    "    'id': d['id'],\n",
    "    'city': d['city'],\n",
    "    'region': d['region'],\n",
    "    'date': d['date'],\n",
    "    'temp_max': d['metadata']['temp_max_celsius'],\n",
    "    'temp_min': d['metadata']['temp_min_celsius'],\n",
    "    'precipitation': d['metadata']['precipitation_mm'],\n",
    "    'wind_speed': d['metadata']['wind_speed_kmh'],\n",
    "    'text_length': len(d['text'].split())\n",
    "} for d in docs])\n",
    "\n",
    "print(\"RESUMEN POR CIUDAD\")\n",
    "print(\"-\" * 80)\n",
    "city_summary = df.groupby('city').agg({\n",
    "    'id': 'count',\n",
    "    'temp_max': ['min', 'mean', 'max'],\n",
    "    'precipitation': 'sum'\n",
    "}).round(2)\n",
    "city_summary.columns = ['Documentos', 'T.Máx Min', 'T.Máx Prom', 'T.Máx Max', 'Lluvia Total']\n",
    "print(city_summary)\n",
    "\n",
    "print(\"\\nRESUMEN POR REGIÓN\")\n",
    "print(\"-\" * 80)\n",
    "region_summary = df.groupby('region').agg({\n",
    "    'id': 'count',\n",
    "    'temp_max': 'mean',\n",
    "    'precipitation': 'sum'\n",
    "}).round(2)\n",
    "region_summary.columns = ['Documentos', 'Temp. Prom. (°C)', 'Lluvia Total (mm)']\n",
    "print(region_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c580b146",
   "metadata": {},
   "source": [
    "## 4. Tokenización y Análisis Léxico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc3fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Tokeniza y normaliza (lowercasing).\"\"\"\n",
    "    tokens = [t.lower() for t in word_tokenize(text, language=\"spanish\") \n",
    "              if t.isalpha()]\n",
    "    return tokens\n",
    "\n",
    "def top_k_tokens(docs, k=20, remove_stop=True, remove_high_freq=False, high_freq_threshold=None):\n",
    "    \"\"\"Extrae los k tokens más frecuentes.\"\"\"\n",
    "    ctr = Counter()\n",
    "    for d in docs:\n",
    "        toks = tokenize(d[\"text\"])\n",
    "        if remove_stop:\n",
    "            toks = [t for t in toks if t not in ALL_STOP]\n",
    "        ctr.update(toks)\n",
    "    \n",
    "    if remove_high_freq and high_freq_threshold is not None:\n",
    "        freqs = sorted(ctr.values())\n",
    "        threshold_val = np.percentile(freqs, high_freq_threshold)\n",
    "        ctr = Counter({token: count for token, count in ctr.items() \n",
    "                      if count <= threshold_val})\n",
    "    \n",
    "    return ctr.most_common(k)\n",
    "\n",
    "print(\"TOKENIZACIÓN COMPLETADA\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Método: NLTK word_tokenize (español)\")\n",
    "print(f\"Normalización: lowercasing + filtro alfabético\")\n",
    "print(f\"Stopwords: NLTK español + términos de dominio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221b059a",
   "metadata": {},
   "source": [
    "## 5. Top-20 Tokens (Análisis de Frecuencias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513bcd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-20 sin stopwords\n",
    "print(\"\\nTOP-20 TOKENS (SIN STOPWORDS)\")\n",
    "print(\"=\" * 60)\n",
    "top_all = top_k_tokens(docs, k=20, remove_stop=True)\n",
    "for i, (token, freq) in enumerate(top_all, 1):\n",
    "    pct = (freq / stats['total_words']) * 100\n",
    "    print(f\"{i:2d}. {token:20s} -> {freq:5d} ({pct:5.2f}%)\")\n",
    "\n",
    "# Top-20 sin stopwords ni alta frecuencia\n",
    "print(\"\\nTOP-20 TOKENS (SIN STOPWORDS NI ALTA FRECUENCIA >P90)\")\n",
    "print(\"=\" * 60)\n",
    "top_filtered = top_k_tokens(docs, k=20, remove_stop=True, \n",
    "                            remove_high_freq=True, high_freq_threshold=90)\n",
    "for i, (token, freq) in enumerate(top_filtered, 1):\n",
    "    pct = (freq / stats['total_words']) * 100\n",
    "    print(f\"{i:2d}. {token:20s} -> {freq:5d} ({pct:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976ba6ac",
   "metadata": {},
   "source": [
    "## 6. Visualización: Distribución de Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d11fd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de top-20 tokens\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Top-20 sin filtrado\n",
    "tokens, freqs = zip(*top_all)\n",
    "ax1.barh(range(len(tokens)), freqs, color='steelblue')\n",
    "ax1.set_yticks(range(len(tokens)))\n",
    "ax1.set_yticklabels(tokens)\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_xlabel('Frecuencia')\n",
    "ax1.set_title('Top-20 Tokens (Sin Stopwords)', fontsize=14, fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Top-20 filtrado\n",
    "tokens_f, freqs_f = zip(*top_filtered)\n",
    "ax2.barh(range(len(tokens_f)), freqs_f, color='coral')\n",
    "ax2.set_yticks(range(len(tokens_f)))\n",
    "ax2.set_yticklabels(tokens_f)\n",
    "ax2.invert_yaxis()\n",
    "ax2.set_xlabel('Frecuencia')\n",
    "ax2.set_title('Top-20 Tokens (Filtrado >P90)', fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/top_tokens_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"✓ Gráfico guardado: outputs/top_tokens_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11822301",
   "metadata": {},
   "source": [
    "## 7. Nubes de Palabras Generadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9450536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar nubes de palabras generadas\n",
    "from IPython.display import Image, display\n",
    "\n",
    "wordclouds = [\n",
    "    ('outputs/wordcloud_sin_filtrado.png', 'Nube sin filtrado (Todas las palabras)'),\n",
    "    ('outputs/wordcloud_sin_stopwords.png', 'Nube sin stopwords (Palabras significativas)'),\n",
    "    ('outputs/wordcloud_filtrado.png', 'Nube filtrada (Sin stopwords ni alta frecuencia)')\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (path, title) in enumerate(wordclouds):\n",
    "    img = Image(filename=path)\n",
    "    axes[idx].imshow(img.data)\n",
    "    axes[idx].set_title(title, fontsize=12, fontweight='bold')\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNUBES DE PALABRAS GENERADAS:\")\n",
    "print(\"=\" * 60)\n",
    "for path, title in wordclouds:\n",
    "    print(f\"  ✓ {title}\")\n",
    "    print(f\"     -> {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88ea509",
   "metadata": {},
   "source": [
    "## 8. Análisis por Región Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2452b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis por mes\n",
    "df['month'] = pd.to_datetime(df['date']).dt.month\n",
    "df['month_name'] = pd.to_datetime(df['date']).dt.strftime('%B')\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Temperatura por mes\n",
    "monthly_temp = df.groupby('month')[['temp_max', 'temp_min']].mean()\n",
    "axes[0, 0].plot(monthly_temp.index, monthly_temp['temp_max'], marker='o', label='T. Máxima', linewidth=2)\n",
    "axes[0, 0].plot(monthly_temp.index, monthly_temp['temp_min'], marker='s', label='T. Mínima', linewidth=2)\n",
    "axes[0, 0].fill_between(monthly_temp.index, monthly_temp['temp_min'], monthly_temp['temp_max'], alpha=0.2)\n",
    "axes[0, 0].set_xlabel('Mes')\n",
    "axes[0, 0].set_ylabel('Temperatura (°C)')\n",
    "axes[0, 0].set_title('Temperatura Promedio por Mes')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Precipitación por mes\n",
    "monthly_precip = df.groupby('month')['precipitation'].sum()\n",
    "axes[0, 1].bar(monthly_precip.index, monthly_precip.values, color='steelblue', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Mes')\n",
    "axes[0, 1].set_ylabel('Precipitación Total (mm)')\n",
    "axes[0, 1].set_title('Precipitación Acumulada por Mes')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Viento por mes\n",
    "monthly_wind = df.groupby('month')['wind_speed'].mean()\n",
    "axes[1, 0].bar(monthly_wind.index, monthly_wind.values, color='coral', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Mes')\n",
    "axes[1, 0].set_ylabel('Velocidad Promedio (km/h)')\n",
    "axes[1, 0].set_title('Velocidad del Viento Promedio por Mes')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Documentos por ciudad\n",
    "city_counts = df['city'].value_counts()\n",
    "axes[1, 1].barh(city_counts.index, city_counts.values, color='green', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Número de Documentos')\n",
    "axes[1, 1].set_title('Distribución de Documentos por Ciudad')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/analisis_temporal.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"✓ Gráfico guardado: outputs/analisis_temporal.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b4d1de",
   "metadata": {},
   "source": [
    "## 9. Métricas Léxicas Avanzadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed68415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulario único sin stopwords\n",
    "all_tokens = []\n",
    "for doc in docs:\n",
    "    toks = tokenize(doc['text'])\n",
    "    toks = [t for t in toks if t not in ALL_STOP]\n",
    "    all_tokens.extend(toks)\n",
    "\n",
    "unique_vocab = set(all_tokens)\n",
    "vocab_size = len(unique_vocab)\n",
    "ttr = vocab_size / len(all_tokens)  # Type-Token Ratio\n",
    "hapax = sum(1 for t in unique_vocab if all_tokens.count(t) == 1)\n",
    "\n",
    "print(\"MÉTRICAS LÉXICAS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Tamaño de vocabulario único: {vocab_size} palabras\")\n",
    "print(f\"Tokens totales: {len(all_tokens)} palabras\")\n",
    "print(f\"Type-Token Ratio (TTR): {ttr:.4f}\")\n",
    "print(f\"Hapax legomena (palabras únicas): {hapax} ({100*hapax/vocab_size:.1f}%)\")\n",
    "print(\"\\nInterpretación:\")\n",
    "print(f\"  - TTR bajo ({ttr:.4f}) indica corpus muy repetitivo\")\n",
    "  f\"(causado por generación automática de texto)\")\n",
    "print(f\"  - {hapax} palabras aparecen solo una vez\")\n",
    "print(f\"  - Vocabulario concentrado en ~{vocab_size} palabras clave\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
